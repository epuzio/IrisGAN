{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vP9R-4I4qNJQ",
        "outputId": "e767587d-872b-441a-ffca-7a1c4c2d20d6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time for epoch 7 is 78.13900589942932 sec\n"
          ]
        }
      ],
      "source": [
        "#imports\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import losses\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "import cv2\n",
        "import os\n",
        "import time\n",
        "from IPython import display\n",
        "from tensorflow.keras import mixed_precision\n",
        "from google.colab import files\n",
        "import zipfile\n",
        "policy = mixed_precision.Policy('mixed_float16')\n",
        "mixed_precision.set_global_policy(policy)\n",
        "\n",
        "\n",
        "## Arguments for GAN\n",
        "EPOCHS = 300\n",
        "EXAMPLES_TO_GENERATE = 8\n",
        "IMAGE_HEIGHT = 512\n",
        "IMAGE_WIDTH = 512\n",
        "BATCH_SIZE = 8\n",
        "TF_RECORD_PATH = \"output.tfrecord\"\n",
        "noise_dim = 100\n",
        "seed = tf.random.normal([EXAMPLES_TO_GENERATE, noise_dim])\n",
        "\n",
        "alpha_Discriminator = 0.2\n",
        "alpha_Generator = 0.2\n",
        "momentum_BatchNormalization = 0.8\n",
        "\n",
        "################# Building Generator #################\n",
        "def add_generator_layer(model, num_filters, kernel, num_strides): #from rectgan\n",
        "  '''\n",
        "  Adding Conv2DTranspose -> BatchNormalization -> LeakyReLU layers to the model\n",
        "  '''\n",
        "  model.add(layers.Conv2DTranspose(num_filters, kernel_size=kernel, strides=num_strides, padding='same', use_bias=False))\n",
        "  model.add(layers.BatchNormalization(momentum=momentum_BatchNormalization)) #instance normalization\n",
        "  model.add(layers.LeakyReLU())\n",
        "\n",
        "def make_generator():\n",
        "  '''\n",
        "  Creating noise for the generator.\n",
        "  '''\n",
        "  model = tf.keras.Sequential() #add layers to generator model\n",
        "  model.add(layers.Dense(16*16*512, input_shape=(100,), use_bias=False))\n",
        "  model.add(layers.BatchNormalization())\n",
        "  model.add(layers.LeakyReLU(alpha=0.2))\n",
        "  model.add(layers.Reshape((16, 16, 512)))\n",
        "\n",
        "  add_generator_layer(model, 512, (5, 5), num_strides=(2, 2))\n",
        "  add_generator_layer(model, 256, (5, 5), num_strides=(2, 2))\n",
        "  add_generator_layer(model, 128, (5, 5), num_strides=(2, 2))\n",
        "  add_generator_layer(model, 64, (5, 5), num_strides=(2, 2))\n",
        "  add_generator_layer(model, 32, (5, 5), num_strides=(2, 2))\n",
        "  model.add(layers.Conv2DTranspose(3,kernel_size=3,strides=1,padding='same',use_bias=False,kernel_initializer=tf.keras.initializers.RandomNormal(stddev=0.02)))\n",
        "  model.add(layers.Activation('tanh'))\n",
        "  print(\"model summarY\", model.summary())\n",
        "  assert model.output_shape == (None, 512, 512, 3), \"Generator output dimensions should be (512, 512, 3), aborting.\"\n",
        "  return model\n",
        "\n",
        "\n",
        "\n",
        "def add_discriminator_layer(model, filters, kernel, num_strides):\n",
        "  model.add(layers.GaussianNoise(0.1)) #reddit tip\n",
        "  model.add(layers.Conv2D(filters, kernel_size=kernel, strides=num_strides, padding='same', use_bias=False))\n",
        "  model.add(layers.LeakyReLU(alpha_Discriminator))\n",
        "  model.add(layers.Dropout(0.3))\n",
        "\n",
        "def make_discriminator():\n",
        "  \"\"\"\n",
        "  Building the discriminator.\n",
        "  \"\"\"\n",
        "  model = tf.keras.Sequential()\n",
        "  add_discriminator_layer(model, 8, (3, 3), num_strides=(2,2))\n",
        "  add_discriminator_layer(model, 16, (3, 3), num_strides=(2,2))\n",
        "  add_discriminator_layer(model, 32, (3, 3), num_strides=(2,2))\n",
        "  add_discriminator_layer(model, 64, (3, 3), num_strides=(2,2))\n",
        "  add_discriminator_layer(model, 128, (3, 3), num_strides=(2,2))\n",
        "  add_discriminator_layer(model, 256, (3, 3), num_strides=(2,2))\n",
        "  add_discriminator_layer(model, 512, (3, 3), num_strides=(2,2))\n",
        "\n",
        "  model.add(layers.Flatten())\n",
        "  # model.add(layers.Dense(128))\n",
        "  model.add(layers.Dense(1))\n",
        "  return model\n",
        "\n",
        "def discriminator_loss(real_output, fake_output):\n",
        "  real_loss = tf.keras.losses.BinaryCrossentropy(from_logits=True)(tf.ones_like(real_output), real_output) #real_output = 1\n",
        "  fake_loss = tf.keras.losses.BinaryCrossentropy(from_logits=True)(tf.zeros_like(fake_output), fake_output) #fake_output = 0\n",
        "  total_loss = real_loss + fake_loss\n",
        "  return total_loss\n",
        "\n",
        "def generator_loss(fake_output):\n",
        "  return tf.keras.losses.BinaryCrossentropy(from_logits=True)(tf.ones_like(fake_output), fake_output) #array of 1s\n",
        "\n",
        "######## Running the model ########\n",
        "\n",
        "\n",
        "@tf.function #turns into a graph, for faster execution\n",
        "def train_step(image_batch, generator, discriminator, generator_optimizer, discriminator_optimizer):\n",
        "    noise = tf.random.normal([BATCH_SIZE, noise_dim])\n",
        "    image_batch = tf.cast(image_batch, tf.float32) / 255.0 #added to catch type mismatch\n",
        "    #compute gradients for discriminator and generator using two different GradientTapes\n",
        "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
        "      generated_images = generator(noise, training=True)\n",
        "\n",
        "      real_output = discriminator(image_batch, training=True)\n",
        "      fake_output = discriminator(generated_images, training=True)\n",
        "\n",
        "      gen_loss = generator_loss(fake_output)\n",
        "      disc_loss = discriminator_loss(real_output, fake_output)\n",
        "\n",
        "    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
        "    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
        "\n",
        "    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
        "    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))\n",
        "\n",
        "def generate_and_save_images(model, epoch, test_input):\n",
        "    predictions = model(test_input, training=False)\n",
        "    for i in range(predictions.shape[0]):  #for each image in the batch\n",
        "        # Convert predictions to float32 before scaling and converting to uint8\n",
        "        image = tf.cast(predictions[i, :, :, :] * 0.5 + 0.5, tf.float32) * 255\n",
        "        image = tf.cast(image, tf.uint8).numpy()\n",
        "        cv2.imwrite(f\"GAN_output_images/img{i}/image{i}_epoch_{epoch:04d}.png\", image)\n",
        "\n",
        "def make_output_directories(examples_to_generate):\n",
        "  '''\n",
        "  Make one image output directory for each example to generate.\n",
        "  This saves all work-in-progress images to each directory.\n",
        "  '''\n",
        "  if not os.path.exists('GAN_output_images'):\n",
        "    os.makedirs('GAN_output_images')\n",
        "  for i in range(examples_to_generate):\n",
        "    new_dir = 'GAN_output_images/img' + str(i)\n",
        "    if not os.path.exists(new_dir):\n",
        "      os.makedirs(new_dir)\n",
        "\n",
        "def decode_image(image):\n",
        "  '''\n",
        "  Map the image to the [-1, 1] range.\n",
        "  '''\n",
        "  image = tf.image.decode_jpeg(image, channels=3)\n",
        "  image = tf.reshape(image, [IMAGE_HEIGHT, IMAGE_WIDTH, 3])\n",
        "  image = (tf.cast(image, tf.float32) / 127.5) - 1 #Normalize images to [-1, 1]\n",
        "  return image\n",
        "\n",
        "\n",
        "def read_tfrecord(example):\n",
        "  '''\n",
        "  Decode the image from the tfrecord file using decode_image\n",
        "  '''\n",
        "  tfrecord_format = {\n",
        "    \"image\": tf.io.FixedLenFeature([], tf.string) #feature name for rectangular_houses\n",
        "  }\n",
        "\n",
        "  example = tf.io.parse_single_example(example, tfrecord_format)\n",
        "  # image = decode_image(example['image'])\n",
        "  image = tf.image.decode_image(example['image'], channels=3)\n",
        "  return image\n",
        "\n",
        "def train():\n",
        "    # Load and prepare the dataset\n",
        "    print(\"make tfr\")\n",
        "    dataset = tf.data.TFRecordDataset(TF_RECORD_PATH)\n",
        "    dataset = dataset.map(read_tfrecord)\n",
        "    dataset_size = sum(1 for _ in dataset)\n",
        "    train_dataset = dataset.shuffle(buffer_size=10000).batch(BATCH_SIZE).prefetch(tf.data.experimental.AUTOTUNE)\n",
        "\n",
        "    print(\"make generator+discriminator\")\n",
        "    # Build generator and discriminator\n",
        "    generator = make_generator()\n",
        "    discriminator = make_discriminator()\n",
        "\n",
        "    print(\"make optimizers\")\n",
        "    # Set up optimizers\n",
        "    generator_optimizer = tf.keras.optimizers.Adam(1e-4)\n",
        "    discriminator_optimizer = tf.keras.optimizers.Adam(1e-4)\n",
        "\n",
        "    # Set up checkpoints\n",
        "    checkpoint_dir = './training_checkpoints'\n",
        "    checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
        "    checkpoint = tf.train.Checkpoint(generator_optimizer=generator_optimizer,\n",
        "                                     discriminator_optimizer=discriminator_optimizer,\n",
        "                                     generator=generator,\n",
        "                                     discriminator=discriminator)\n",
        "\n",
        "    print(\"beginning training loop\")\n",
        "    # Training loop\n",
        "    for epoch in range(EPOCHS):\n",
        "        start = time.time()\n",
        "\n",
        "        for image_batch in train_dataset:\n",
        "            train_step(image_batch, generator, discriminator, generator_optimizer, discriminator_optimizer)\n",
        "\n",
        "        # Produce images for the GIF\n",
        "        if (epoch) % 2 == 0:\n",
        "          print(\"saving image...\")\n",
        "          display.clear_output(wait=True)\n",
        "          generate_and_save_images(generator, epoch + 1, seed)\n",
        "\n",
        "\n",
        "        # Save the model every 5 epochs\n",
        "        if (epoch + 1) % 5 == 0:\n",
        "            print(\"saving checkpoint  ...\")\n",
        "            checkpoint.save(file_prefix=checkpoint_prefix)\n",
        "\n",
        "        print(f'Time for epoch {epoch + 1} is {time.time() - start} sec')\n",
        "        # Clear the session at the end of each epoch\n",
        "        tf.keras.backend.clear_session()\n",
        "\n",
        "    # Generate after the final epoch\n",
        "    display.clear_output(wait=True)\n",
        "    generate_and_save_images(generator, EPOCHS, seed)\n",
        "\n",
        "def main():\n",
        "  device_name = tf.test.gpu_device_name()\n",
        "  if device_name != '/device:GPU:0':\n",
        "    raise SystemError('GPU device not found')\n",
        "  print('Found GPU at: {}'.format(device_name))\n",
        "  with tf.device('/device:GPU:0'):\n",
        "    train()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "  main()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}